{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toUOy6gxHCov",
        "outputId": "49f0ac43-d27b-4fe9-9f32-55f82f6ea069"
      },
      "outputs": [],
      "source": [
        "# CODE FOR CREATING AND TRAINING CHATBOT\n",
        "\n",
        "# import libraries\n",
        "import random\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from keras.models import Sequential\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.optimizers import SGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9M2ufN_KOoi"
      },
      "outputs": [],
      "source": [
        "# load json file with tags, patterns & responses\n",
        "\n",
        "intents = json.loads(open(\"intents.json\").read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_Rh4cCoKQkn"
      },
      "outputs": [],
      "source": [
        "# create lists to store data\n",
        "words = []\n",
        "labels = []\n",
        "docs = []\n",
        "ignore_letters = [\"?\", \"!\", \".\", \",\"]\n",
        "\n",
        "# separate words from patterns and add to words list\n",
        "for intent in intents['intents']:\n",
        "\tfor pattern in intent['patterns']:\n",
        "\t\tword_list = nltk.word_tokenize(pattern)\n",
        "\t\twords.extend(word_list) \n",
        "\t\t\n",
        "\t\t# associate patterns with respective tags\n",
        "\t\tdocs.append(((word_list), intent['tag']))\n",
        "\n",
        "\t\t# append tags to labels list\n",
        "\t\tif intent['tag'] not in labels:\n",
        "\t\t\tlabels.append(intent['tag'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuV3qsziKasy"
      },
      "outputs": [],
      "source": [
        "# store root words (lemma)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words = [lemmatizer.lemmatize(word)\n",
        "\t\tfor word in words if word not in ignore_letters]\n",
        "words = sorted(set(words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmH-nKoxKdYW"
      },
      "outputs": [],
      "source": [
        "# save words and labels lists to binary files\n",
        "\n",
        "pickle.dump(words, open('words.pkl', 'wb'))\n",
        "pickle.dump(labels, open('labels.pkl', 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06rNtlpJHMJ0",
        "outputId": "e09441f2-5c37-452d-a26d-19840c59417e"
      },
      "outputs": [],
      "source": [
        "# binarise data for neural network processing:\n",
        "# (0 = word not in pattern, 1 = word in pattern)\n",
        "\n",
        "training = []\n",
        "output_empty = [0]*len(labels)\n",
        "for doc in docs:\n",
        "\tbag = []\n",
        "\tword_patterns = doc[0]\n",
        "\tword_patterns = [lemmatizer.lemmatize(\n",
        "\t\tword.lower()) for word in word_patterns]\n",
        "\tfor word in words:\n",
        "\t\tbag.append(1) if word in word_patterns else bag.append(0)\n",
        "  # shuffle training data\n",
        "\toutput_row = list(output_empty)\n",
        "\toutput_row[labels.index(doc[1])] = 1\n",
        "\ttraining.append([bag, output_row])\n",
        "random.shuffle(training)\n",
        "# Change starts here\n",
        "random.shuffle(training)\n",
        "\n",
        "train_x = []\n",
        "train_y = []\n",
        "\n",
        "for pair in training:\n",
        "    train_x.append(pair[0])\n",
        "    train_y.append(pair[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_rJJmPKKhn3"
      },
      "outputs": [],
      "source": [
        "# split data using np array\n",
        "train_x = np.array(train_x)\n",
        "train_y = np.array(train_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXZ_mIfyHNiT"
      },
      "outputs": [],
      "source": [
        "# create Sequential Neural Network model\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_shape=(len(train_x[0]), ),\n",
        "\t\t\t\tactivation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(train_y[0]), activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWz37tFCKmeb",
        "outputId": "8da0a255-915f-43ec-ccb3-11250753aae2"
      },
      "outputs": [],
      "source": [
        "# compile model\n",
        "sgd = SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "\t\t\toptimizer=sgd, metrics=['accuracy'])\n",
        "hist = model.fit(np.array(train_x), np.array(train_y),\n",
        "\t\t\t\tepochs=200, batch_size=5, verbose=1)\n",
        "\n",
        "print(\"Training complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTSF_RVaKpnu"
      },
      "outputs": [],
      "source": [
        "# saving the model\n",
        "model.save(\"chatbotmodel.h5\", hist)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
